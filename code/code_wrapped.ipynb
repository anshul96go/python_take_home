{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59dfc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except:\n",
    "    !conda install -c conda-forge py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a09861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns;\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "    \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6035f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test,self.y_pred = None,None,None,None,None\n",
    "        self.model = None\n",
    "    \n",
    "    \n",
    "    def feature_selection(self):\n",
    "        \n",
    "        # select features with missing values less than 60%\n",
    "        req_cols, final_req_cols = [],[]\n",
    "        X = self.X\n",
    "        for col in self.X.columns:\n",
    "            miss_rate = round(X[col].isna().sum()/len(X),2)\n",
    "            if miss_rate < 0.6:\n",
    "                req_cols.append(col)\n",
    "        \n",
    "        # select features with correlation > 0.2\n",
    "        '''\n",
    "        Improvements:\n",
    "        1. Add visualizations\n",
    "        2. Create different correlations for different type of variables (refer notebook)\n",
    "        '''\n",
    "        df = X[req_cols]\n",
    "        df['target'] = self.y\n",
    "#         g = sns.pairplot(df,hue = 'target', diag_kind= 'hist',\n",
    "#                      vars=df.columns[:-1],\n",
    "#                      plot_kws=dict(alpha=0.5), \n",
    "#                      diag_kws=dict(alpha=0.5))\n",
    "#         plt.show()\n",
    "        corr_matrix = df.corr()\n",
    "        for col in req_cols:\n",
    "            if abs(corr_matrix[\"target\"][col])>0.2:\n",
    "                final_req_cols.append(col)\n",
    "\n",
    "        # update X dataframe which contain only selected features\n",
    "        self.X = X[final_req_cols]\n",
    "        \n",
    "    \n",
    "    def data_split(self, split=0.2):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X,self.y,train_size=split)\n",
    "        \n",
    "    \n",
    "    def data_normalization(self):\n",
    "        cols = self.X.columns\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(self.X)\n",
    "        self.X = pd.DataFrame(scaler.transform(self.X))\n",
    "        self.X.columns = cols\n",
    "    \n",
    "    def logistic_regression(self):\n",
    "        self.model = LogisticRegression()\n",
    "    \n",
    "    \n",
    "    def decision_tree(self):\n",
    "        self.model = DecisionTreeClassifier()\n",
    "    \n",
    "    \n",
    "    def multinomial_naive_bayes(self):\n",
    "        self.model = MultinomialNB()\n",
    "       \n",
    "    \n",
    "    def gaussian_naive_bayes(self):\n",
    "        self.model = GaussianNB()\n",
    "    \n",
    "    \n",
    "    def knn(self):\n",
    "        self.model = KNeighborsClassifier()\n",
    "    \n",
    "    \n",
    "    def rf(self,n_trees=100,criteria='gini',max_depth=None):\n",
    "        self.model = RandomForestClassifier(n_estimators=n_trees, criterion=criteria, max_depth=max_depth)\n",
    "    \n",
    "    \n",
    "    def xgb(self):\n",
    "        self.model = XGBClassifier(objective=\"binary:logistic\")\n",
    "    \n",
    "    \n",
    "    def svm(self):\n",
    "        self.model = SVC(gamma='auto')\n",
    "    \n",
    "    def gradient_boost(self,n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0):\n",
    "        self.model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate,max_depth=max_depth, random_state=random_state)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def parameter_tuning(self,model='knn',scoring='accuracy',cv=5,given_params=False):\n",
    "        \n",
    "        # check if the parameter grid is given or have to use the the default one\n",
    "        if given_params==False:\n",
    "            # use predefined parameters for each model\n",
    "            if model=='knn':\n",
    "                self.knn()\n",
    "                params = [{'n_neighbors':[3,5,7,9], \n",
    "                           'weights':['uniform','distance'],\n",
    "                           'leaf_size':[15,20,30]}] \n",
    "            if model=='logistic_regression':\n",
    "                self.logistic_regression()\n",
    "                params = [{'penalty':['none','l2','elasticnet','l1'], 'C':[0.001,0.01,0.1,1,10,100,1000], 'fit_intercept':[True,False]}]\n",
    "            if model== \"decision_tree\":\n",
    "                self.decision_tree()\n",
    "                params = [{'criterion':['gini','entropy'],'max_depth':[3,5,10,15,20,50]}]\n",
    "            if model=='multinomial_naive_bayes':\n",
    "                self.multinomial_naive_bayes()\n",
    "                params = [{'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]}]\n",
    "            if model==\"gaussian_naive_bayes\":\n",
    "                self.gaussian_naive_bayes()\n",
    "                print(\"Message: No hyperparameter to tune for gaussian naive bayes, use predict() function to get predictions!\")\n",
    "                return\n",
    "            if model==\"rf\":\n",
    "                self.rf()\n",
    "                params = [{'n_estimators':[10,50,100,200],\n",
    "                           'criterion':['gini','entropy'],\n",
    "                           'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                           'max_depth':[3,5,10,20]}]\n",
    "            if model==\"xgb\":\n",
    "                self.xgb()\n",
    "                params=[{'max_depth': [3,6,9,12],\n",
    "                        'subsample': [0.8,0.9,1.0]}]\n",
    "            if model=='svm':\n",
    "                self.svm()\n",
    "                params = [{'C': [1, 10], 'kernel': ('linear', 'rbf')}]\n",
    "            \n",
    "        # use parameter grid given\n",
    "        else:\n",
    "            params = given_params\n",
    "            if model=='knn':\n",
    "                self.knn()\n",
    "            if model=='logistic_regression':\n",
    "                self.logistic_regression()\n",
    "            if model== \"decision_tree\":\n",
    "                self.decision_tree()\n",
    "            if model=='multinomial_naive_bayes':\n",
    "                self.multinomial_naive_bayes()\n",
    "            if model==\"gaussian_naive_bayes\":\n",
    "                self.gaussian_naive_bayes()\n",
    "                print(\"Message: No hyperparameter to tune for gaussian naive bayes, use predict() function to get predictions!\")\n",
    "                return        \n",
    "            if model==\"rf\":\n",
    "                self.rf()\n",
    "            if model==\"xgb\":\n",
    "                self.xgb()\n",
    "            if model=='svm':\n",
    "                self.svm()\n",
    "                \n",
    "        # initialise grid search\n",
    "        gs = GridSearchCV(estimator=self.model,\n",
    "                  param_grid = params,\n",
    "                  scoring=scoring,\n",
    "                  cv=cv,\n",
    "                  verbose=0)\n",
    "        \n",
    "        \n",
    "        # fit the data and get results\n",
    "        try:\n",
    "            gs.fit(self.X_train,self.y_train)\n",
    "            print(\"best params: \",gs.best_params_)\n",
    "            print(\"score: \",gs.score(self.X_train,self.y_train))\n",
    "            self.model = gs\n",
    "        except:\n",
    "            print(\"Message: The parameters you entered doesn't match the input format. Please refer to the parameter_tuning function to understand the input format for parameter ranges\")\n",
    "            return\n",
    "        \n",
    "\n",
    "    def predict(self):\n",
    "        # fit/train the model\n",
    "        clf = self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # make predictions\n",
    "        self.y_pred = clf.predict(self.X_test)\n",
    "    \n",
    "    \n",
    "    def performance(self,threshold=0.5):\n",
    "        '''\n",
    "        Improvements\n",
    "        1. Add visualisations\n",
    "        2. Read and explain the performannce matrix\n",
    "        '''\n",
    "        \n",
    "        \n",
    "#         # convert probability to binary output using given threshold (parameter)\n",
    "#         y_pred_binary = (self.y_pred>threshold).astype(int)\n",
    "#         print(y_pred_binary)\n",
    "#         print(self.y_pred)\n",
    "        \n",
    "        # accuracy\n",
    "        accuracy = accuracy_score(self.y_pred, self.y_test)\n",
    "        print(\"accuracy:\",accuracy)\n",
    "        \n",
    "        # confusion mat/rix\n",
    "        cm = confusion_matrix(self.y_pred, self.y_test)\n",
    "        print(\"confusion matrix:\\n\",cm)\n",
    "        \n",
    "        # roc_auc\n",
    "        roc_auc = roc_auc_score(self.y_pred, self.y_test)\n",
    "        print(\"ROC AUC:\",roc_auc)\n",
    "        \n",
    "        # pr_auc\n",
    "        pr_auc = average_precision_score(self.y_pred, self.y_test)\n",
    "        print(\"PR AUC:\",pr_auc)\n",
    "        \n",
    "        return {'accuracy':accuracy, 'confusion_matrix':cm, 'roc_auc':roc_auc, 'pr_auc':pr_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data='data_file_path'):\n",
    "    \n",
    "    # read the data\n",
    "    data = pd.read_csv('../input/heart.csv')\n",
    "    X = data[data.columns[:-1]]\n",
    "    y = data[data.columns[-1]]\n",
    "    \n",
    "    # with parameter tuning\n",
    "\n",
    "    # call class\n",
    "    p = Prediction(X,y)\n",
    "\n",
    "    # data normalization\n",
    "    p.data_normalization()\n",
    "\n",
    "    # feature engineering\n",
    "    p.feature_selection()\n",
    "\n",
    "    # split data into train and test\n",
    "    p.data_split()\n",
    "\n",
    "    # parameter tuning\n",
    "    p.parameter_tuning(model='svm')\n",
    "\n",
    "    # make predictions\n",
    "    p.predict()\n",
    "\n",
    "    # get model performance\n",
    "    performance = p.performance()\n",
    "    \n",
    "    performance.to_txt('')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b03db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--run-model RUN_MODEL] [--run-all RUN_ALL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/anshulg/Library/Jupyter/runtime/kernel-3a1a6070-50cb-43bf-969f-c65c421a5f3a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize parser\n",
    "msg = \"Classification Problem\"\n",
    "parser = argparse.ArgumentParser(description = msg)\n",
    "parser.add_argument(\"--run-model\", help = \"Run only the model given as parameter\")\n",
    "parser.add_argument(\"--run-all\", help = \"Run all models\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Main script for ML.\"\n",
    "parser = argparse.ArgumentParser(description = msg)\n",
    "parser.add_argument(\"--collect-data\", help = \"Prepare Train and Test data. Creates csv files.\", action='store_true')\n",
    "parser.add_argument(\"--train\", help = \"Train\", action='store_true')\n",
    "parser.add_argument(\"--model-path\", help = \"Model path to use. Pass 'new' to create new model.\")\n",
    "parser.add_argument(\"--confusion\", help = \"Calculate confusion table for the given data (in csv format).\")\n",
    "parser.add_argument(\"--plots\", help = \"Plot using the evaluation tar file generated while training\")\n",
    "parser.add_argument(\"--predict\", help = \"Predict using the given data (in csv format).\")\n",
    "args = parser.parse_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
